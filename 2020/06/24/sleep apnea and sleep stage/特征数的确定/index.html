
<!DOCTYPE html>
<html lang="zh-CN" class="loading">
<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>特征数的确定 - ESY</title>
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="google" content="notranslate" />
    <meta name="keywords" content="ESY,"> 
    <meta name="description" content="miao,特征数的确定经过50次平均值筛选后，可看出对应3分期时，特征数为15,4、5分期的特征数都为23，此时为最高的特征值。

为了确定特征数对准确率的影响，第一个方案
选择(10-22)个特征，将每个特,"> 
    <meta name="author" content="esy"> 
    <link rel="alternative" href="atom.xml" title="ESY" type="application/atom+xml"> 
    <link rel="icon" href="/img/hhh.png"> 
    <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
    <link rel="stylesheet" href="/css/diaspora.css">
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
         (adsbygoogle = window.adsbygoogle || []).push({
              google_ad_client: "ca-pub-8691406134231910",
              enable_page_level_ads: true
         });
    </script>
    <script async custom-element="amp-auto-ads"
        src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>
</head>

<body class="loading">
    <span id="config-title" style="display:none">ESY</span>
    <div id="loader"></div>
​    <div id="single">
    <div id="top" style="display: block;">
    <div class="bar" style="width: 0;"></div>
    <a class="icon-home image-icon" href="javascript:;" target="_blank" rel="noopener" data-url="https://esyyes.github.io"></a>
    <div title="播放/暂停" class="icon-play"></div>
    <h3 class="subtitle">特征数的确定</h3>
    <div class="social">
        <!--<div class="like-icon">-->
            <!--<a href="javascript:;" target="_blank" rel="noopener" class="likeThis active"><span class="icon-like"></span><span class="count">76</span></a>-->
        <!--</div>-->
        <div>
            <div class="share">
                <a title="获取二维码" class="icon-scan" href="javascript:;" target="_blank" rel="noopener"></a>
            </div>
            <div id="qr"></div>
        </div>
    </div>
    <div class="scrollbar"></div>
</div>

    <div class="section">
        <div class="article">
    <div class='main'>
        <h1 class="title">特征数的确定</h1>
        <div class="stuff">
            <span>六月 24, 2020</span>
            
  <ul class="post-tags-list" itemprop="keywords"><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/work/" rel="tag">work</a></li></ul>


        </div>
        <div class="content markdown">
            <h1 id="特征数的确定"><a href="#特征数的确定" class="headerlink" title="特征数的确定"></a>特征数的确定</h1><p>经过50次平均值筛选后，可看出对应3分期时，特征数为15,4、5分期的特征数都为23，此时为最高的特征值。</p>
<p><img src="/images/features_section.png" alt="features section"></p>
<p>为了确定特征数对准确率的影响，第一个方案</p>
<p>选择(10-22)个特征，将每个特征都用于分类，然后将对比看看分类的结果</p>
<h2 id="特征缩减"><a href="#特征缩减" class="headerlink" title="特征缩减"></a>特征缩减</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time     : 2020/6/17</span></span><br><span class="line"><span class="comment"># @Author   : esy</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> data_preprocessing <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> classifiers <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"></span><br><span class="line">warnings.filterwarnings(<span class="string">"ignore"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> index <span class="keyword">in</span> range(<span class="number">10</span>, <span class="number">22</span>):</span><br><span class="line">    class_scores = []</span><br><span class="line">    kappa_scores = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">19</span>):</span><br><span class="line">        feature = pd.read_excel(<span class="string">'D:/zccode/all_feature'</span> + <span class="string">'/features'</span> + <span class="string">'%s'</span> % i + <span class="string">'.xlsx'</span>)</span><br><span class="line">        data = pd.get_dummies(feature.iloc[<span class="number">0</span>:len(feature), <span class="number">1</span>:])</span><br><span class="line">        note = pd.read_excel(<span class="string">'D:/zccode/all_note'</span> + <span class="string">'/note'</span> + <span class="string">'%s'</span> % i + <span class="string">'.xlsx'</span>)</span><br><span class="line">        tag = pd.get_dummies(note.iloc[<span class="number">0</span>:len(data), <span class="number">1</span>:])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 选定固定的特征值</span></span><br><span class="line">        feature_import = pd.read_excel(<span class="string">'D:/zccode'</span> + <span class="string">'/feature_important'</span> + <span class="string">'%s'</span> % <span class="number">2</span> + <span class="string">'.xlsx'</span>)</span><br><span class="line">        df = pd.get_dummies(feature_import.iloc[<span class="number">0</span>:len(feature_import), <span class="number">1</span>:index])</span><br><span class="line"></span><br><span class="line">        std_data = data_pre(data[df.keys()])</span><br><span class="line">        label = pd.get_dummies(tag.iloc[<span class="number">0</span>:len(data), <span class="number">1</span>:<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">        class_score = []</span><br><span class="line">        kappa_score = []</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">50</span>):</span><br><span class="line">            X_train, X_test, y_train, y_test = train_test_split(std_data, label, test_size=<span class="number">0.3</span>)</span><br><span class="line">            score = run_classifiers(X_train, X_test, y_train, y_test)</span><br><span class="line">            class_score.append(score[<span class="number">0</span>])</span><br><span class="line">            kappa_score.append(score[<span class="number">1</span>])</span><br><span class="line">        class_scores.append([(np.array([class_score[a][k] <span class="keyword">for</span> a <span class="keyword">in</span> range(<span class="number">50</span>)])).mean() <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">9</span>)])</span><br><span class="line">        kappa_scores.append([(np.array([kappa_score[a][k] <span class="keyword">for</span> a <span class="keyword">in</span> range(<span class="number">50</span>)])).mean() <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">9</span>)])</span><br><span class="line">        print(<span class="string">f'已经运行<span class="subst">&#123;i&#125;</span>次'</span>)</span><br><span class="line"></span><br><span class="line">    class_total = pd.DataFrame(class_scores)</span><br><span class="line">    kappa_total = pd.DataFrame(kappa_scores)</span><br><span class="line">    class_total.to_excel(<span class="string">'no RFE classf'</span> + <span class="string">'%d'</span> % index + <span class="string">".xlsx"</span>)</span><br><span class="line">    kappa_total.to_excel(<span class="string">'no RFE kappaf'</span> + <span class="string">'%d'</span> % index + <span class="string">".xlsx"</span>)</span><br></pre></td></tr></table></figure>

<p>此次方案，在进行分类时，直接选用的是固定的特征，即迭代50次特征贡献度，平均后的结果，用固定特征对应的特征数，去进行分类。</p>
<h2 id="分类模型"><a href="#分类模型" class="headerlink" title="分类模型"></a>分类模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time     : 2020/6/15</span></span><br><span class="line"><span class="comment"># @Author   : esy</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVC</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.neural_network <span class="keyword">import</span> MLPClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> cohen_kappa_score</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_rf</span><span class="params">(train_x, test_x, train_y, test_y)</span>:</span></span><br><span class="line">    clf = RandomForestClassifier()</span><br><span class="line">    clf.fit(train_x, train_y)</span><br><span class="line">    pred_y = clf.predict(test_x)</span><br><span class="line">    acr = accuracy_score(test_y, pred_y)</span><br><span class="line">    kappa = cohen_kappa_score(test_y, pred_y)</span><br><span class="line">    <span class="keyword">return</span> [acr, kappa]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_gbt</span><span class="params">(train_x, test_x, train_y, test_y)</span>:</span></span><br><span class="line">    clf = GradientBoostingClassifier()</span><br><span class="line">    clf.fit(train_x, train_y)</span><br><span class="line">    pred_y = clf.predict(test_x)</span><br><span class="line">    acr = accuracy_score(test_y, pred_y)</span><br><span class="line">    kappa = cohen_kappa_score(test_y, pred_y)</span><br><span class="line">    <span class="keyword">return</span> [acr, kappa]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_knn</span><span class="params">(train_x, test_x, train_y, test_y)</span>:</span></span><br><span class="line">    clf = KNeighborsClassifier()</span><br><span class="line">    clf.fit(train_x, train_y)</span><br><span class="line">    pred_y = clf.predict(test_x)</span><br><span class="line">    acr = accuracy_score(test_y, pred_y)</span><br><span class="line">    kappa = cohen_kappa_score(test_y, pred_y)</span><br><span class="line">    <span class="keyword">return</span> [acr, kappa]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_dt</span><span class="params">(train_x, test_x, train_y, test_y)</span>:</span></span><br><span class="line">    clf = DecisionTreeClassifier()</span><br><span class="line">    clf.fit(train_x, train_y)</span><br><span class="line">    pred_y = clf.predict(test_x)</span><br><span class="line">    acr = accuracy_score(test_y, pred_y)</span><br><span class="line">    kappa = cohen_kappa_score(test_y, pred_y)</span><br><span class="line">    <span class="keyword">return</span> [acr, kappa]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_svm</span><span class="params">(train_x, test_x, train_y, test_y)</span>:</span></span><br><span class="line">    clf = SVC()</span><br><span class="line">    clf.fit(train_x, train_y)</span><br><span class="line">    pred_y = clf.predict(test_x)</span><br><span class="line">    acr = accuracy_score(test_y, pred_y)</span><br><span class="line">    kappa = cohen_kappa_score(test_y, pred_y)</span><br><span class="line">    <span class="keyword">return</span> [acr, kappa]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_lsvm</span><span class="params">(train_x, test_x, train_y, test_y)</span>:</span></span><br><span class="line">    clf = LinearSVC()</span><br><span class="line">    clf.fit(train_x, train_y)</span><br><span class="line">    pred_y = clf.predict(test_x)</span><br><span class="line">    acr = accuracy_score(test_y, pred_y)</span><br><span class="line">    kappa = cohen_kappa_score(test_y, pred_y)</span><br><span class="line">    <span class="keyword">return</span> [acr, kappa]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_lr</span><span class="params">(train_x, test_x, train_y, test_y)</span>:</span></span><br><span class="line">    clf = LogisticRegression()</span><br><span class="line">    clf.fit(train_x, train_y)</span><br><span class="line">    pred_y = clf.predict(test_x)</span><br><span class="line">    acr = accuracy_score(test_y, pred_y)</span><br><span class="line">    kappa = cohen_kappa_score(test_y, pred_y)</span><br><span class="line">    <span class="keyword">return</span> [acr, kappa]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_ml</span><span class="params">(train_x, test_x, train_y, test_y)</span>:</span></span><br><span class="line">    clf = MLPClassifier()</span><br><span class="line">    clf.fit(train_x, train_y)</span><br><span class="line">    pred_y = clf.predict(test_x)</span><br><span class="line">    acr = accuracy_score(test_y, pred_y)</span><br><span class="line">    kappa = cohen_kappa_score(test_y, pred_y)</span><br><span class="line">    <span class="keyword">return</span> [acr, kappa]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_sgd</span><span class="params">(train_x, test_x, train_y, test_y)</span>:</span></span><br><span class="line">    clf = SGDClassifier()</span><br><span class="line">    clf.fit(train_x, train_y)</span><br><span class="line">    pred_y = clf.predict(test_x)</span><br><span class="line">    acr = accuracy_score(test_y, pred_y)</span><br><span class="line">    kappa = cohen_kappa_score(test_y, pred_y)</span><br><span class="line">    <span class="keyword">return</span> [acr, kappa]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 直接运行结果</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_classifiers</span><span class="params">(train_x, test_x, train_y, test_y)</span>:</span></span><br><span class="line">    rf = run_rf(train_x, test_x, train_y, test_y)</span><br><span class="line">    gbt = run_gbt(train_x, test_x, train_y, test_y)</span><br><span class="line">    knn = run_knn(train_x, test_x, train_y, test_y)</span><br><span class="line">    dt = run_dt(train_x, test_x, train_y, test_y)</span><br><span class="line">    svm = run_svm(train_x, test_x, train_y, test_y)</span><br><span class="line">    lsvm = run_lsvm(train_x, test_x, train_y, test_y)</span><br><span class="line">    lr = run_lr(train_x, test_x, train_y, test_y)</span><br><span class="line">    ml = run_ml(train_x, test_x, train_y, test_y)</span><br><span class="line">    sgd = run_sgd(train_x, test_x, train_y, test_y)</span><br><span class="line"></span><br><span class="line">    class_s = [sgd[<span class="number">0</span>], svm[<span class="number">0</span>], lsvm[<span class="number">0</span>], lr[<span class="number">0</span>], knn[<span class="number">0</span>], dt[<span class="number">0</span>], rf[<span class="number">0</span>], gbt[<span class="number">0</span>], ml[<span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line">    kappa_s = [sgd[<span class="number">1</span>], svm[<span class="number">1</span>], lsvm[<span class="number">1</span>], lr[<span class="number">1</span>], knn[<span class="number">1</span>], dt[<span class="number">1</span>], rf[<span class="number">1</span>], gbt[<span class="number">1</span>], ml[<span class="number">1</span>]]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> [class_s, kappa_s]</span><br></pre></td></tr></table></figure>

<p>分类模型对应的为以下9种，并分别计算出的kappa值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[sgd[<span class="number">0</span>], svm[<span class="number">0</span>], lsvm[<span class="number">0</span>], lr[<span class="number">0</span>], knn[<span class="number">0</span>], dt[<span class="number">0</span>], rf[<span class="number">0</span>], gbt[<span class="number">0</span>], ml[<span class="number">0</span>]]</span><br></pre></td></tr></table></figure>

<p>文件保存为： E:\sleep_stage</p>
<p>例如：no RFE class10.xlsx</p>
<p><strong>10其实对应的是9，在index中，而且不一定对应的是9还可能是对应的8，这点待会儿还需要检验下，直接进行最高的和牺牲对比的</strong></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>对比结果发现，特征越多，准确率越高，在其他分类模型中，随机森林相对变化不大</p>
<h2 id="方案二"><a href="#方案二" class="headerlink" title="方案二"></a>方案二</h2><h3 id="最大准确率"><a href="#最大准确率" class="headerlink" title="最大准确率"></a>最大准确率</h3><p>虽然此时的max对应的特征数已经确定，但是可以明显看出，从特征数7-9这些开始已经没有太多的明显变化</p>
<p>所以为了进一步的缩减特征值，在此次方案中采取牺牲1%的准确率，来降低特征数的方针。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time     : 2020/6/24</span></span><br><span class="line"><span class="comment"># @Author   : esy</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"></span><br><span class="line"><span class="comment"># 忽略警告</span></span><br><span class="line">warnings.filterwarnings(<span class="string">"ignore"</span>)</span><br><span class="line"></span><br><span class="line">all_section = []</span><br><span class="line"><span class="keyword">for</span> q <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">    data = pd.read_excel(<span class="string">'E:/features'</span> + <span class="string">'/themes'</span> + <span class="string">'%s'</span> % q + <span class="string">'.xlsx'</span>)</span><br><span class="line">    train_data = np.array(data).tolist()</span><br><span class="line">    list_mean = [[np.array([eval(train_data[i][k])[j] <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">50</span>)]).mean() <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">25</span>)] <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">18</span>)]</span><br><span class="line">    feature_section = [np.array([list_mean[j][i] * <span class="number">100</span> <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">18</span>)]).mean() <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">25</span>)]</span><br><span class="line">    all_section.append(feature_section)</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">1</span>, <span class="number">26</span>)</span><br><span class="line">class_5 = all_section[<span class="number">0</span>]</span><br><span class="line">class_4 = all_section[<span class="number">1</span>]</span><br><span class="line">class_3 = all_section[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">class_num = [x[all_section[i].index(max(all_section[i]))] <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>)]</span><br><span class="line">class_max = [max(all_section[i]) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>)]</span><br></pre></td></tr></table></figure>

<p>先找到对应的最大值对应的位置class_num和值class_max</p>
<p>[23, 23, 15]</p>
<p>[79.64724146295774, 86.40048338502821, 89.03929260177847]</p>
<p>来个制表符，直接输出，让他看起来好看点</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">    print(<span class="string">f'分期为<span class="subst">&#123;<span class="number">5</span>-i&#125;</span>时'</span>, end=<span class="string">'\t'</span>)</span><br><span class="line">    print(<span class="string">f'最高准确率为%.2f%%'</span> % max(all_section[i]), end=<span class="string">'\t'</span>)</span><br><span class="line">    print(<span class="string">f'特征数为<span class="subst">&#123;x[all_section[i].index(max(all_section[i]))]&#125;</span>'</span>)</span><br></pre></td></tr></table></figure>

<p>输出要%时，即为%%</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">分期为<span class="number">5</span>时	最高准确率为<span class="number">79.65</span>%	特征数为<span class="number">23</span></span><br><span class="line">分期为<span class="number">4</span>时	最高准确率为<span class="number">86.40</span>%	特征数为<span class="number">23</span></span><br><span class="line">分期为<span class="number">3</span>时	最高准确率为<span class="number">89.04</span>%	特征数为<span class="number">15</span></span><br></pre></td></tr></table></figure>

<h3 id="降低1-后对应结果"><a href="#降低1-后对应结果" class="headerlink" title="降低1%后对应结果"></a>降低1%后对应结果</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">f'准确率降低1%后对应的特征数'</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">    print(<span class="string">f'分期为<span class="subst">&#123;<span class="number">5</span>-i&#125;</span>时'</span>, end=<span class="string">'\t'</span>)</span><br><span class="line">    print(<span class="string">f'最高准确率为%.2f%%'</span> % max(all_section[i]), end=<span class="string">'\t'</span>)</span><br><span class="line">    print(<span class="string">f'最高准确率降低1%%后为%.2f%%'</span> % (max(all_section[i]) * <span class="number">0.99</span>), end=<span class="string">'\t'</span>)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(len(all_section[i])):</span><br><span class="line">        <span class="keyword">if</span> (max(all_section[i]) * <span class="number">0.99</span>) &lt; all_section[i][j]:</span><br><span class="line">            print(<span class="string">f'比较筛选的准确率为%.2f%%'</span> % (all_section[i][j]), end=<span class="string">'\t'</span>)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    print(<span class="string">f'特征数为<span class="subst">&#123;x[all_section[i].index(all_section[i][j])]&#125;</span>'</span>)</span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">最大准确率时对应的特征数</span><br><span class="line">分期为<span class="number">5</span>时	最高准确率为<span class="number">79.65</span>%	特征数为<span class="number">23</span></span><br><span class="line">分期为<span class="number">4</span>时	最高准确率为<span class="number">86.40</span>%	特征数为<span class="number">23</span></span><br><span class="line">分期为<span class="number">3</span>时	最高准确率为<span class="number">89.04</span>%	特征数为<span class="number">15</span></span><br><span class="line">准确率降低<span class="number">1</span>%后对应的特征数</span><br><span class="line">分期为<span class="number">5</span>时	最高准确率为<span class="number">79.65</span>%	最高准确率降低<span class="number">1</span>%后为<span class="number">78.85</span>%	比较筛选的准确率为<span class="number">78.86</span>%	特征数为<span class="number">9</span></span><br><span class="line">分期为<span class="number">4</span>时	最高准确率为<span class="number">86.40</span>%	最高准确率降低<span class="number">1</span>%后为<span class="number">85.54</span>%	比较筛选的准确率为<span class="number">85.66</span>%	特征数为<span class="number">8</span></span><br><span class="line">分期为<span class="number">3</span>时	最高准确率为<span class="number">89.04</span>%	最高准确率降低<span class="number">1</span>%后为<span class="number">88.15</span>%	比较筛选的准确率为<span class="number">88.22</span>%	特征数为<span class="number">7</span></span><br></pre></td></tr></table></figure>

<p><img src="http://pic1.win4000.com/wallpaper/1/5450829edeb96.jpg" alt=""></p>

            <!--[if lt IE 9]><script>document.createElement('audio');</script><![endif]-->
            <audio id="audio" loop="1" preload="auto" controls="controls" data-autoplay="false">
                <source type="audio/mpeg" src="">
            </audio>
            
                <ul id="audio-list" style="display:none">
                    
                        
                            <li title='0' data-url='https://link.hhtjim.com/163/509313159.mp3'></li>
                        
                    
                        
                            <li title='1' data-url='https://link.hhtjim.com/163/579954.mp3'></li>
                        
                    
                        
                            <li title='2' data-url='https://link.hhtjim.com/163/579648.mp3'></li>
                        
                    
                        
                            <li title='3' data-url='https://link.hhtjim.com/163/40915694.mp3'></li>
                        
                    
                </ul>
            
        </div>
        

    </div>
    
</div>


    </div>
</div>
</body>
<script src="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<script src="//lib.baomitu.com/jquery/1.8.3/jquery.min.js"></script>
<script src="/js/plugin.js"></script>
<script src="/js/diaspora.js"></script>
<link rel="stylesheet" href="/photoswipe/photoswipe.css">
<link rel="stylesheet" href="/photoswipe/default-skin/default-skin.css">
<script src="/photoswipe/photoswipe.min.js"></script>
<script src="/photoswipe/photoswipe-ui-default.min.js"></script>

<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>
    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">
        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>
        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">
            <div class="pswp__top-bar">
                <!--  Controls are self-explanatory. Order can be changed. -->
                <div class="pswp__counter"></div>
                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
                <button class="pswp__button pswp__button--share" title="Share"></button>
                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>
            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>
            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>
            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>
            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>
        </div>
    </div>
</div>




</html>